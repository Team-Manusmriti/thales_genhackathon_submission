# **Technical documentation for ListenIQ**


## Key Features

### Intelligent Speech Monitoring

- **Real-time Speech-to-Text**: Continuous offline recognition speech capture using [Flutter STT](http://pub.dev/packages/speech_to_text) package
- **Encrypted Storage**: All speech data tokenized, chunked, and stored in encrypted text files using [Flutter Encrypt](https://pub.dev/packages/encrypt) package
- **Privacy-First**: No cloud dependencies, all processing happens on-device

---

## Advanced Object/Action Detection

- **YoloV8s-oiv7 Small Model**: Trained on UCF101 dataset for real-time action recognition
- **Supported Actions**: CricketShot, PlayingCello, Punch, ShavingBeard, TennisSwing
- **MobileNet v2 Processing**: Efficient mobile-optimized inference
- **Context Generation**: Automatic text file creation from detected actions

### Smart RAG Implementation

- **Local Vector Database**: SQLite-based semantic search with encrypted embeddings
- **OLLAMA all-MiniLM-L6-v2**: Converted to TFLite using ai-edge-torch for on-device embeddings
- **Contextual Responses**: LLM-powered answers based solely on user's personal data
- **Structured Prompting**: Intelligent context compilation for accurate responses

### Future-Ready Screen Capture _(In Development)_

- **Periodic Screenshots**: Time-based or pixel-change detection triggers
- **OCR Integration**: Text extraction from captured screens
- **Intelligent Monitoring**: Adaptive capture based on user activity patterns

---

## Technology Stack

### Frontend Framework

- **Flutter**: Cross-platform mobile development
- **Dart**: Primary programming language

### Backend Framework

- **On-device Execution**: No remote backend or APIs were used; all computation (AI inference, embeddings, and retrieval) happens locally on the device.  
- **TensorFlow Lite**: Executes AI/ML models directly on mobile hardware.  
- **Custom Inference Pipelines**: Handles OCR, embeddings, and recall logic without network dependencies.  


### Data & Storage

- **SQLite**: Local vector database with embeddings
- **Encrypted File System**: Secure text file storage
- **permission_handler**: System access management

---

## System Architecture

### Module 1: Speech Intelligence System

```
Speech Input → Flutter STT → Text Processing → Tokenization →
Chunking → Encryption → File Storage → Embedding Generation → Vector DB
```

### Module 2: Action/Object Detection Pipeline

```
Video/Camera Input → YoloV8s-oiv7 Detection → MobileNet v2 Processing →
Action/Object Classification → Context Generation → Text File Creation →
Embedding Storage → Vector Database
```

### Module 3: RAG Query System

```
User Query → Semantic Search (SQLite) → Context Retrieval →
Prompt Structuring → LLM Processing → Contextual Response
```

---

# Model Details & Performance

## YoloV8s-oiv7 Small Configuration
> Note: For customized action detection model, consider using the TensorFlow Lite **(.tflite)** model provided by us on [Hugging Face](https://huggingface.co/NanG01/Action_detection ).

### Dataset Information
| Attribute | Details |
|-----------|---------|
| **Dataset** | UCF101 Video Dataset (Object/Action Recognition) |
| **Total Classes** | 101 action categories |
| **Total Videos** | 13,320 videos from YouTube |
| **Training Data** | 27 hours of video data |
| **Selected Classes** | 5 primary action categories (CricketShot, PlayingCello, Punch, ShavingBeard, TennisSwing) |
| **Dataset Source** | [Kaggle UCF101 Videos](https://www.kaggle.com/datasets/abdallahwagih/ucf101-videos) |
| **Original Paper** | [UCF101: A Dataset of 101 Human Actions Classes From Videos in The Wild](https://arxiv.org/abs/1212.0402) |

### Model Specifications
| Specification | Value |
|---------------|--------|
| **Input Size** | 640×640 pixels |
| **Parameters** | 11.16M parameters |
| **Model Size** | ~22MB (TensorFlow Lite optimized) |
| **mAP val 50-95** | 44.9 |
| **License** | AGPL-3.0 (Open Source) / Enterprise License |

### Performance Metrics
| Platform | Inference Time | Memory Usage |
|----------|----------------|--------------|
| **Mobile CPU (ARM)** | <100ms per frame | <50MB RAM |
| **CPU ONNX** | 128.4ms average | Optimized for mobile |
| **A100 TensorRT** | 1.20ms (server baseline) | High-performance reference |
| **Mobile GPU** | ~50-80ms per frame | Hardware acceleration enabled |

---

## Speech Recognition System

### Vosk API Specifications
| Feature | Details |
|---------|---------|
| **Engine** | Vosk Offline Speech Recognition |
| **Repository** | [alphacep/vosk-api](https://github.com/alphacep/vosk-api) |
| **License** | Apache 2.0 License |
| **Model Size** | 50MB compact models per language |
| **Processing** | Real-time streaming recognition |

### Language Support (20+ Languages)
| Language Category | Supported Languages |
|------------------|-------------------|
| **European** | English, German, French, Spanish, Portuguese, Italian, Dutch, Swedish, Czech, Polish, Greek |
| **Asian** | Chinese (Mandarin), Japanese, Korean, Hindi, Gujarati, Telugu |
| **Middle Eastern** | Arabic, Farsi, Turkish |
| **Others** | Russian, Ukrainian, Kazakh, Vietnamese, Filipino, Esperanto, Breton, Tajik, Uzbek |

### Performance Characteristics
| Metric | Specification |
|--------|--------------|
| **Latency** | Zero-latency streaming (real-time) |
| **Accuracy** | 85-95% (language-dependent) |
| **Processing** | On-device, completely offline |
| **Memory Footprint** | <100MB total (including model) |
| **CPU Usage** | ~5-15% on modern mobile processors |
| **Audio Format** | 16kHz, 16-bit PCM audio |
| **Streaming** | Continuous recognition with partial results |

---

## Audio Processing

### FFmpeg Integration
| Component | Specification |
|-----------|---------------|
| **Framework** | FFmpeg v7.1+ (Flutter integration) |
| **License** | LGPL/GPL License |
| **Download** | [ffmpeg.org/download.html](https://ffmpeg.org/download.html) |
| **Flutter Plugin** | `ffmpeg_kit_flutter` or `flutter_ffmpeg` |

### Capabilities & Features
| Feature | Details |
|---------|---------|
| **Audio Extraction** | Extract audio from video streams in real-time |
| **Format Support** | MP4, AVI, MKV, MP3, AAC, WAV, FLAC, OGG |
| **Codecs** | H.264, H.265, VP8, VP9, AAC, MP3, Opus |
| **Processing** | Audio filtering, volume normalization, noise reduction |
| **Compression** | Variable bitrate encoding for optimal file sizes |
| **Batch Processing** | Multiple file processing with queue management |

### Performance Benchmarks
| Operation | Processing Speed | Resource Usage |
|-----------|-----------------|----------------|
| **Audio Extraction** | 4-6x real-time speed | ~10-15% CPU usage |
| **Format Conversion** | 2-4x real-time speed | <100MB memory |
| **Video Processing** | 1-2x real-time speed | Variable based on resolution |
| **Filter Application** | Near real-time | Minimal additional overhead |

### Flutter Integration

**Key Integration Features**:
- Native Android/iOS library binding
- Asynchronous execution with progress callbacks  
- Custom FFmpeg command construction
- Automatic temporary file management
- Error handling and logging support

---

## Embedding Model

> Note: For optimized version of all-MiniLM-L6-v2, consider using the TensorFlow Lite **(.tflite)** model provided by us on [Hugging Face](https://huggingface.co/Madhur-Prakash-Mangal/all-MiniLM-L6-v2-tflite).

### all-MiniLM-L6-v2 Specifications
| Attribute | Value |
|-----------|-------|
| **Base Model** | sentence-transformers/all-MiniLM-L6-v2 |
| **Official Repository** | [Sentence Transformers](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) |
| **Architecture** | Transformer-based encoder (MiniLM) |
| **Embedding Dimensions** | 384-dimensional dense vectors |
| **Max Sequence Length** | 256 word pieces (tokens) |
| **Parameters** | ~22.7M parameters |
| **License** | Apache 2.0 License |

### Mobile Optimization via ai-edge-torch
| Conversion Step | Technical Details |
|----------------|------------------|
| **Source Framework** | PyTorch/HuggingFace Transformers |
| **Conversion Tool** | Google AI Edge Torch |
| **Official Repository** | [google-ai-edge/ai-edge-torch](https://github.com/google-ai-edge/ai-edge-torch) |
| **Target Format** | TensorFlow Lite (.tflite) |
| **Quantization** | INT8 post-training quantization |
| **Model Size Reduction** | ~4x smaller (90MB → 22MB) |

### Performance Metrics
| Platform | Inference Time | Memory Usage | Throughput |
|----------|----------------|--------------|------------|
| **Mobile CPU (ARM64)** | <50ms per sentence | ~30MB | 20+ sentences/second |
| **Mobile CPU (ARM32)** | ~80ms per sentence | ~35MB | 12+ sentences/second |
| **Desktop CPU** | ~10ms per sentence | ~40MB | 100+ sentences/second |
| **GPU (Mobile)** | ~20ms per sentence | ~45MB | 50+ sentences/second |


### Use Cases in ListenIQ
| Application | Implementation |
|-------------|---------------|
| **Semantic Search** | Query user's personal data with natural language |
| **Content Clustering** | Group similar speech/action contexts automatically |
| **RAG Retrieval** | Find relevant personal memories for contextual responses |
| **Similarity Matching** | Match new experiences with historical patterns |
| **Context Understanding** | Generate embeddings for multi-modal data fusion |

### Privacy & Security
- **On-Device Processing**: All embedding generation happens locally
- **No Cloud Dependencies**: Complete offline operation
- **Encrypted Storage**: Embeddings stored in encrypted SQLite database
- **Local Processing**: Zero data transmission to external servers

---


### Resource Management
| Component | Peak Memory | Storage Requirements | CPU Usage |
|-----------|-------------|---------------------|-----------|
| **YoloV8s-oiv7 Small** | 50MB | 22MB model file | 15-25% |
| **Vosk Speech** | 100MB | 50MB language model | 10-20% |
| **MiniLM Embeddings** | 30MB | 22MB TFLite model | 5-15% |
| **FFmpeg Processing** | 100MB | Temporary files | 20-40% |
| **Total System** | <300MB | <150MB models | <80% |

---

## Large Language Model (LLM) 

> Note: For optimized version of distilgpt2, consider using the TensorFlow Lite **(.tflite)** model provided by us on [Hugging Face](https://huggingface.co/Madhur-Prakash-Mangal/distilgpt2-TFLITE).

### DistilGPT-2 Specifications
| Attribute | Value |
|-----------|-------|
| **Base Model** | distilgpt2 |
| **Official Repository** | [Hugging Face](https://huggingface.co/distilgpt2) |
| **Architecture** | Transformer-based language model |
| **Parameters** | ~82M parameters |
| **License** | Apache 2.0 License |

---

### For more detailed documentation on specific modules, please refer to the following resources:

- [Workflow Overview](/docs/workflow_documentation.md)

- [Object/Action Detection Module](/docs/action-module-README.md)

- [RAG Module](/docs/rag-module-README.md)

- [Speech To Text Module](/docs/speech-module-README.md)

---

## Market Analysis & Scalability
For detailed market analysis, scalability considerations, and potential use cases, please refer to the [Market Overview Document](/docs/Market_Overview.md).

## App UI

| Home Screen | STT Screen | Video Processing Screen | Options Menu |
|-------------|------------|-------------------------|--------------|
| <img src="https://github.com/user-attachments/assets/d7267f1d-0772-482b-b164-3072d106ee61" width="250" style="margin-right:120px;"/> | <img src="https://github.com/user-attachments/assets/a12bc170-5486-461f-8893-c2701808aaf7" width="250"/> | <img src="https://github.com/user-attachments/assets/bb51da72-49f0-489b-9042-8ee1023550cc" width="250" style="margin-right:120px;"/> | <img src="https://github.com/user-attachments/assets/0e83e961-9e58-4aee-9308-8ed2ca50403b" width="250"/> |

---

## Installation & Setup

### Prerequisites

```bash
# Flutter SDK (Latest stable version)
flutter doctor

# Fetch dependencies
flutter pub get
```

### Key Dependencies

```yaml
dependencies:
  flutter:
    sdk: flutter
  go_router: ^16.2.0     # Routing
  
  # Permissions
  permission_handler: ^12.0.1      # Requesting runtime permissions
  speech_to_text: ^7.3.0      # speech recognition
  
  avatar_glow: ^3.0.1     # mic background visual
 
  # Machine Learning
  sqflite: ^2.4.2         # SQLite database
  encrypt: ^5.0.3            # AES decryption helper
  collection: ^1.19.1           # Collection utilities
  huggingface_dart: ^0.0.2     # Hugging Face API

  flutter_tts: ^4.2.3      # Text-to-speech

  # Camera and video processing
  camera: ^0.11.2      # Camera
  video_player: ^2.10.0  # Video player
  image: ^4.5.4          # Image processing

  # File handling
  file_picker: ^10.3.2  # File picker
  path_provider: ^2.1.5    # path provider
  path: ^1.9.1     # path of files

  # HTTP and Networking
  http: ^1.5.0  # HTTP client
  dio: ^5.9.0  # HTTP client

  # UI and State Management
  cupertino_icons: ^1.0.8 
  lucide_icons_flutter: ^3.0.9       # Icons
  flutter_riverpod: ^2.6.1       # State management

  # UI and utilities
  intl: ^0.20.2                  # Internationalization
  shared_preferences: ^2.5.3      # Shared preferences
  flutter_launcher_icons: ^0.14.4  # Flutter launcher icons
  flutter_spinkit: ^5.2.2          # Loading indicators

  # Audio processing
  flutter_sound: ^9.28.0       # Audio recording and processing
  audioplayers: ^6.5.0         # Audio playback

  # TensorFlow Lite
  tflite_flutter: ^0.11.0        # TFLite inference
  tflite_flutter_helper: ^0.3.1    # TFLite helper functions

   # For advanced video processing
  ffmpeg_kit_flutter_new: ^3.2.0

```
---

### Model Setup

Follow the steps below to set up the required models:


1. **YoloV8s-oiv7 Small (Object/Action Recognition)**  
   [Download from Hugging Face](https://huggingface.co/Ultralytics/YOLOv8)  
    - For **TFLite version**, use the model provided by us on [Hugging Face](https://huggingface.co/NanG01/Action_detection).

2. **Vosk Speech Recognition Models**  
   [Install from GitHub](https://github.com/alphacep/vosk-api)  

3. **all-MiniLM-L6-v2 (TFLite version)**  
   [Download from Hugging Face](https://huggingface.co/Madhur-Prakash-Mangal/all-MiniLM-L6-v2-tflite)  

4. **distilgpt2 (TFLite version)**  
   [Download from Hugging Face](https://huggingface.co/Madhur-Prakash-Mangal/distilgpt2-TFLITE)  

4. **Place all downloaded models** in `assets/models/` directory

---

### Asset Configuration

Ensure that the asset files are correctly referenced in your `pubspec.yaml`:

```yaml
flutter:
  assets:
    - assets/models/
```
---

### Permissions Configuration

```xml
<!-- Android Manifest -->
<uses-permission android:name="android.permission.RECORD_AUDIO" />
<uses-permission android:name="android.permission.CAMERA" />
<uses-permission android:name="android.permission.WRITE_EXTERNAL_STORAGE" />
```

---

### Run the Application

```bash
flutter run
```

---

## Thales GenTech India Hackathon 2025 Alignment

### Challenge Requirements

- **On-Device AI**: Complete offline functionality with zero cloud dependencies
- **Privacy-First**: No personal data leaves the device
- **Real-Time Processing**: Immediate response without latency
- **Multi-Modal Intelligence**: Speech, vision, and text processing integration

---

### Innovation Highlights

- **Novel RAG Implementation**: First-of-its-kind mobile RAG with encrypted local storage
- **Multi-Modal Fusion**: Seamless integration of speech, object, and action detection
- **Edge AI Optimization**: Advanced model quantization and mobile-specific optimizations
- **Privacy by Design**: Built-in encryption and local-only processing

---

## Future Roadmap

### Phase 1: Screen Intelligence _(Currently Under Development)_

- **Adaptive Screenshot System**: Smart capture based on user activity patterns
- **Advanced OCR Integration**: Multi-language text extraction capabilities
- **Pixel Change Detection**: Intelligent monitoring with configurable sensitivity
- **Context-Aware Capture**: Activity-based screenshot frequency adjustment

---

### Phase 2: Enhanced AI Models

- **Expanded Action Recognition**: Support for 50+ action categories
- **Emotion Detection**: Facial expression and voice sentiment analysis
- **Advanced NLP**: Local language model for complex query understanding
- **Multi-Language Support**: Comprehensive international language coverage

---

### Phase 3: Community & Scaling

- **Open Source Components**: Release privacy-preserving AI modules
- **Developer SDK**: Tools for building similar applications
- **Performance Optimization**: Advanced quantization and hardware acceleration
- **Cross-Platform Support**: Web and desktop implementations

---

*Part of ListenIQ - Built with ❤️ for Thales GenTech India Hackathon 2025*
---
